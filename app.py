import streamlit as st
from PIL import Image, ImageOps
import numpy as np
import cv2
from io import BytesIO
from ultralytics import YOLO
import os

# from dotenv import load_dotenv
# load_dotenv() # .env íŒŒì¼ì—ì„œ í™˜ê²½ë³€ìˆ˜ ë¶ˆëŸ¬ì˜¤ê¸°

# -----------------------------
# ì„¤ì •
# -----------------------------
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
MODEL_DIR = os.path.join(BASE_DIR, "models")

st.set_page_config(page_title="AI ì¦ëª…ì‚¬ì§„ ìƒì„±ê¸°", layout="centered")
st.title("ðŸ“¸ AI ì¦ëª…ì‚¬ì§„ ìƒì„±ê¸°")
st.markdown("ì‚¬ë§ê³¼ ë°˜ë ¤ë™ë¬¼ì˜ ì‚¬ì§„ì„ ì¦ëª…ì‚¬ì§„ìœ¼ë¡œ ìžë™ ë³€í™”í•´ë³´ì„¸ìš”!")
st.title(":camera_with_flash: AI ì¦ëª…ì‚¬ì§„ ìƒì„±ê¸°")
st.markdown("ì‚¬ëžŒê³¼ ë°˜ë ¤ë™ë¬¼ì˜ ì‚¬ì§„ì„ ì¦ëª…ì‚¬ì§„ìœ¼ë¡œ ìžë™ ë³€í™˜í•´ë³´ì„¸ìš”!")

# -----------------------------
# ì‚¬ì´ë“œë°” ì„¤ì •
# -----------------------------
st.sidebar.header("âš™ï¸ ì„¤ì •")

st.sidebar.subheader("ðŸ“¦ ëª¨ë¸ ì„ íƒ")
model_choice = st.sidebar.selectbox("ëª¨ë¸ ì„ íƒ", ["ì‚¬ëžŒ (ê¸°ë³¸)", "dog-cat-detector"])


target = st.sidebar.selectbox("ëŒ€ìƒì„ ì„ íƒí•˜ì„¸ìš”", ["ì„ íƒí•˜ì„¸ìš”", "ì‚¬ëžŒ", "ê°•ì•„ì§€/ê³ ì–‘ì´"], index=0)
if target == "ì„ íƒí•˜ì„¸ìš”":
    target = None

# [ìˆ˜ì •ë¨] ì–¼êµ´ ê°ì§€ ë°©ë²• ì„ íƒ ì œê±°ë¨ (YOLOë§Œ ì‚¬ìš©)
bg_color = st.sidebar.selectbox("ë°°ê²½ ìƒ‰ìƒ ì„ íƒ", ["í°ìƒ‰", "íŒŒëž€ìƒ‰", "ë¯¼íŠ¸"])
st.sidebar.markdown("## ðŸ“„ ì¶”ê°€ ì •ë³´")
name = st.sidebar.text_input("ì´ë¦„ (ì„ íƒ)")
breed = st.sidebar.text_input("ì¢… / ì¶œìƒì¼ ë“± (ì„ íƒ)")
target = st.sidebar.selectbox("ëŒ€ìƒì„ ì„ íƒí•˜ì„¸ìš”", ["ì‚¬ëžŒ", "ê°•ì•„ì§€", "ê³ ì–‘ì´"])

if target == "ì‚¬ëžŒ":
    gender = st.sidebar.radio("ì„±ë³„ì„ ì„ íƒí•˜ì„¸ìš”", ["ì—¬ìž", "ë‚¨ìž"])

# -----------------------------
# íŒŒì¼ ì—…ë¡œë”
# -----------------------------
uploaded_file = None
if target:
    uploaded_file = st.file_uploader("ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”", type=["jpg", "jpeg", "png"])
else:
    st.info("ðŸ“Œ ë¨¼ì € **ëŒ€ìƒì„ ì„ íƒ**í•˜ë©´ ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•  ìˆ˜ ìžˆì–´ìš”!")



# -----------------------------
# ëª¨ë¸ ë¡œë“œ
# -----------------------------
@st.cache_resource
def load_model(model_choice):
    if model_choice == "dog-cat-detector":
        model_path = os.path.join(MODEL_DIR, "dog-cat-detector.pt")
    else:
        st.error("ì‚¬ëžŒ ëª¨ë¸ì€ ì•„ì§ ì§€ì›ë˜ì§€ ì•Šì•„ìš”.")
        return None

    if not os.path.exists(model_path):
        st.error(f"ëª¨ë¸ íŒŒì¼ì´ ì¡´ìž¬í•˜ì§€ ì•Šì•„ìš”: {model_path}")
        return None

    return YOLO(model_path)  # ultralyticsì—ì„œ YOLO ëª¨ë¸ ë¡œë”©

# -----------------------------
# IOU ì¤‘ë³µ ë°•ìŠ¤ ì œê±°
# -----------------------------
# def non_max_suppression(boxes, iou_threshold=0.5):
#     if len(boxes) == 0:
#         return []

#     boxes = sorted(boxes, key=lambda x: (x[2] - x[0]) * (x[3] - x[1]), reverse=True)
#     selected_boxes = []

#     while boxes:
#         chosen_box = boxes.pop(0)
#         selected_boxes.append(chosen_box)

#         def iou(box1, box2):
#             x1, y1, x2, y2 = box1
#             x1_p, y1_p, x2_p, y2_p = box2
#             xi1, yi1 = max(x1, x1_p), max(y1, y1_p)
#             xi2, yi2 = min(x2, x2_p), min(y2, y2_p)
#             intersection = max(0, xi2 - xi1) * max(0, yi2 - yi1)
#             area1 = (x2 - x1) * (y2 - y1)
#             area2 = (x2_p - x1_p) * (y2_p - y1_p)
#             union = area1 + area2 - intersection
#             return intersection / union if union > 0 else 0

#         boxes = [box for box in boxes if iou(chosen_box, box) < iou_threshold]

#     return selected_boxes

# -----------------------------
# ê°•ì•„ì§€/ê³ ì–‘ì´ ì–¼êµ´ ê°ì§€ ë° ì¦ëª…ì‚¬ì§„ ìƒì„±
# -----------------------------
def process_pet(image, model_choice):

    model = load_model(model_choice)
    if model is None:
        st.warning("ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return


    img_np = np.array(image)
    img_bgr = cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR)

    with st.spinner("ë°˜ë ¤ë™ë¬¼ ì–¼êµ´ ì¸ì‹ ì¤‘... â³"):
        results = model.predict(source=img_bgr, save=False, verbose=False)
        boxes = results[0].boxes.xyxy.cpu().numpy().astype(int)

    #boxes = non_max_suppression(boxes, iou_threshold=0.5)

    if len(boxes) == 0:
        st.warning("ë°˜ë ¤ë™ë¬¼ ì–¼êµ´ì„ ì°¾ì§€ ëª»í–ˆì–´ìš” ðŸ˜¢")
        return

    st.markdown("### ðŸŸ© ì¸ì‹ëœ ë°˜ë ¤ë™ë¬¼ ì–¼êµ´ (ì„ íƒ ê°€ëŠ¥)")
    cropped_faces = []
    boxed_image = img_np.copy()

    for i, (x1, y1, x2, y2) in enumerate(boxes):
        box_width = x2 - x1
        desired_aspect = 3 / 4
        desired_crop_height = int(box_width / desired_aspect)

        new_y1 = y1
        new_y2 = min(y1 + desired_crop_height, img_np.shape[0])

        if new_y2 - new_y1 < 50:
            continue

        cv2.rectangle(boxed_image, (x1, new_y1), (x2, new_y2), (0, 255, 0), 2)
        cropped_faces.append(img_np[new_y1:new_y2, x1:x2])

    st.image(boxed_image, caption="ê°ì§€ëœ ì–¼êµ´", use_container_width=True)

    if len(cropped_faces) == 0:
        st.warning("ì–¼êµ´ ì¤‘ì‹¬ ì˜ì—­ì´ ë„ˆë¬´ ìž‘ì•„ì„œ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´ìš”.")
        return

    selected_face = st.selectbox("ë³€í™˜í•  ì–¼êµ´ì„ ì„ íƒí•˜ì„¸ìš”", list(range(1, len(cropped_faces) + 1)))
    selected = cropped_faces[selected_face - 1]

    st.markdown("#### âœ… ì„ íƒí•œ ì–¼êµ´ ë¯¸ë¦¬ë³´ê¸°")
    st.image(selected, caption=f"ì„ íƒí•œ ì–¼êµ´ #{selected_face}", use_container_width=False)

    if selected.shape[0] < 50 or selected.shape[1] < 50:
        st.warning("ê°ì§€ëœ ì–¼êµ´ì´ ë„ˆë¬´ ìž‘ì•„ í’ˆì§ˆì´ ë–¨ì–´ì§ˆ ìˆ˜ ìžˆì–´ìš” ðŸ˜…")

    face_image = Image.fromarray(selected)

    if st.button("ðŸ“¸ ì¦ëª…ì‚¬ì§„ ìƒì„±í•˜ê¸° âœ¨"):
        display_result_section(image, face_image, target="ë°˜ë ¤ë™ë¬¼")



# -----------------------------
# í…Œë‘ë¦¬ ì¶”ê°€
# -----------------------------
def add_border(image, border_size, border_color=(0, 0, 0)):
    if isinstance(image, Image.Image):
        image = np.array(image)
    bordered_image = cv2.copyMakeBorder(
        image, border_size, border_size, border_size, border_size,
        cv2.BORDER_CONSTANT, value=border_color
    )
    return Image.fromarray(bordered_image)

# -----------------------------
# ê²°ê³¼ í‘œì‹œ
# -----------------------------
def display_result_section(original_img, result_img, target="ëŒ€ìƒ"):
    result_img_with_border = add_border(result_img, border_size=1)
    st.success(f"{target}ì˜ ì¦ëª…ì‚¬ì§„ ìƒì„± ì™„ë£Œ!")
    col1, col2 = st.columns(2)
    with col1:
        st.markdown("#### â¬…ï¸ Before")
        st.image(original_img, use_column_width=True)
    with col2:
        st.markdown("#### âž¡ï¸ After")
        st.image(result_img_with_border, use_column_width=True)

    buf = BytesIO()
    result_img_with_border.save(buf, format="PNG")
    st.download_button("ðŸ“¥ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ", data=buf.getvalue(), file_name="id_photo.png", mime="image/png")

# -----------------------------
# ì‚¬ëžŒ ì²˜ë¦¬
# -----------------------------
def process_human(image):
    st.info("ì‚¬ëžŒ ì–¼êµ´ ì¸ì‹ ê¸°ëŠ¥ì€ ê³§ ì¶”ê°€ë  ì˜ˆì •ìž…ë‹ˆë‹¤!")

# -----------------------------
# ë©”ì¸ ì‹¤í–‰
# -----------------------------
if uploaded_file:
    try:
        image = Image.open(uploaded_file).convert("RGB")
        st.markdown("### ðŸ–¼ï¸ ì›ë³¸ ì´ë¯¸ì§€")
        st.image(image, caption="ì—…ë¡œë“œëœ ì´ë¯¸ì§€", use_column_width=True)

        if target == "ì‚¬ëžŒ":
            process_human(image)
        elif target == "ê°•ì•„ì§€/ê³ ì–‘ì´":
            process_pet(image, model_choice=model_choice)

    except Exception as e:
        st.error(f"ì´ë¯¸ì§€ ë¡œë”© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
# ì´ë¯¸ì§€ ì—…ë¡œë“œ
uploaded_file = st.file_uploader("ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”", type=["jpg", "jpeg", "png"])

if uploaded_file is not None:
    file_bytes = np.asarray(bytearray(uploaded_file.read()), dtype=np.uint8)
    image = cv2.imdecode(file_bytes, 1)
    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

    h, w, _ = image.shape
    bounding_boxes = []

    if target == "ì‚¬ëžŒ":
        face_detection = mp_face_detection.FaceDetection(model_selection=1, min_detection_confidence=0.8)
        results = face_detection.process(image_rgb)
        if results.detections:
            for detection in results.detections:
                bboxC = detection.location_data.relative_bounding_box
                x_min = int(bboxC.xmin * w)
                y_min = int(bboxC.ymin * h)
                width = int(bboxC.width * w)
                height = int(bboxC.height * h)
                expand_ratio_height_top = 0.8
                expand_ratio_height_bottom = 0.8
                expand_ratio_width = 0.2
                new_y_min = max(0, y_min - int(height * expand_ratio_height_top))
                new_y_max = min(h, y_min + height + int(height * expand_ratio_height_bottom))
                new_x_min = max(0, x_min - int(width * expand_ratio_width))
                new_x_max = min(w, x_min + int(width * (1 + expand_ratio_width)))
                bounding_boxes.append((new_x_min, new_y_min, new_x_max - new_x_min, new_y_max - new_y_min))
    else:
        class_id = 1 if target == "ê°•ì•„ì§€" else 0  # YOLO class ID
        results = yolo_model.predict(image_rgb, conf=0.8)
        for box, cls in zip(results[0].boxes.xyxy, results[0].boxes.cls):
            if int(cls) == class_id:
                x1, y1, x2, y2 = map(int, box[:4])
                x1 = max(0, x1 - 10)
                y1 = max(0, y1 - 30)
                x2 = min(w, x2 + 10)
                y2 = min(h, y2 + 30)
                bounding_boxes.append((x1, y1, x2 - x1, y2 - y1))

    # GrabCut ë°°ê²½ ì œê±° í•¨ìˆ˜
    def remove_background(image, bounding_box):
        mask = np.zeros(image.shape[:2], np.uint8)
        bgd_model = np.zeros((1, 65), np.float64)
        fgd_model = np.zeros((1, 65), np.float64)
        x, y, w, h = bounding_box
        rect = (x, y, w, h)
        cv2.grabCut(image, mask, rect, bgd_model, fgd_model, iterCount=5, mode=cv2.GC_INIT_WITH_RECT)
        mask_2d = np.where((mask == 2) | (mask == 0), 0, 1).astype("uint8")
        result_image = image * mask_2d[:, :, np.newaxis]
        return result_image
    
    if bounding_boxes:
        cols = st.columns(len(bounding_boxes))
        selected_box_index = 0
        with st.container():
            for i, col in enumerate(cols):
                if col.button(f"ì–¼êµ´ {i+1}"):
                    selected_box_index = i
                selected_box = bounding_boxes[selected_box_index]
                cropped_face_with_bg_removed = remove_background(image_rgb.copy(), selected_box)
                st.image(cropped_face_with_bg_removed, caption="ë°°ê²½ ì œê±°ëœ ì–¼êµ´", use_column_width=True)

        # ì¦ëª…ì‚¬ì§„ ìƒì„± ë²„íŠ¼ í•­ìƒ ë³´ì´ê²Œ
        if st.button("ì¦ëª…ì‚¬ì§„ ìƒì„±í•˜ê¸° :sparkles:"):
            try:
                input_image_path = "background_removed_face.png"
                cropped_face_pil = Image.fromarray(cropped_face_with_bg_removed)
                cropped_face_pil.save(input_image_path)

                if target == "ì‚¬ëžŒ":
                    if gender == "ì—¬ìž":
                        prompt_description = "A professional ID photo of a asian woman wearing a suit on a clean white background."
                    elif gender == "ë‚¨ìž":
                        prompt_description = "A professional ID photo of a asian man wearing a suit on a clean white background."
                elif target == "ê°•ì•„ì§€":
                    prompt_description = "A cute professional portrait of a dog wearing a bow tie on a studio background."
                elif target == "ê³ ì–‘ì´":
                    prompt_description = "A charming ID photo of a cat in a tuxedo on a clean white background."

                response = openai.Image.create(
                    prompt=prompt_description,
                    n=1,
                    size="1024x1024"
                )

                generated_image_url = response["data"][0]["url"]
                st.success("ì¦ëª…ì‚¬ì§„ì´ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤!")

                response_img_data = requests.get(generated_image_url)
                output_image_path = "generated_id_photo.png"
                with open(output_image_path, "wb") as output_file:
                    output_file.write(response_img_data.content)

                final_image_pil = Image.open(output_image_path)
                resized_image_pil = final_image_pil.resize((413, 531))
                resized_output_path = "resized_id_photo.png"
                resized_image_pil.save(resized_output_path)

                st.image(resized_image_pil, caption="Resized ID Photo (413x531)", use_column_width=True)
                with open(resized_output_path, "rb") as file:
                    st.download_button(
                        label=":inbox_tray: ì¦ëª…ì‚¬ì§„ ë‹¤ìš´ë¡œë“œ",
                        data=file,
                        file_name="id_photo.png",
                        mime="image/png",
                    )
            except Exception as e:
                st.error(f"ì¦ëª…ì‚¬ì§„ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        else:
            st.warning("ë¨¼ì € ì–¼êµ´ì„ ì¸ì‹í•œ í›„ ì¦ëª…ì‚¬ì§„ì„ ìƒì„±í•˜ì„¸ìš”.")
    else:
        st.warning("ì–¼êµ´ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì´ë¯¸ì§€ë¥¼ ì‹œë„í•´ë³´ì„¸ìš”.")
else:
    st.info("ì¢Œì¸¡ ì‚¬ì´ë“œë°”ì—ì„œ ì„¤ì • í›„ ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•´ ì£¼ì„¸ìš”.")
